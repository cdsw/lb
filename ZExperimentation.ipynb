{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipfshttpclient as ip\n",
    "import os\n",
    "cli = ip.connect()\n",
    "print(cli.id()['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cli.add('Zdumx') # STORE THING INSIDE IPFS\n",
    "hh = res['Hash'] # GET HASH\n",
    "# cli.cat(hh) # get the textual stuff | CANNOT RETURN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = './test' # this is a FOLDER\n",
    "cli.get(hh, location) # put the hash into that folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('./test/'+hh) # DELETE LATER\n",
    "cli.close() # CLOSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 3, 7\n",
    "b = 4, 8\n",
    "\n",
    "c = []\n",
    "c.append(a)\n",
    "c.append(b)\n",
    "\n",
    "for k, f in c:\n",
    "    print(k, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EMPTY MODEL FOLDER\n",
    "ls = os.listdir('./models')\n",
    "for x in ls:\n",
    "    os.remove('./models/' + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class A:\n",
    "    def __init__(self, p):\n",
    "        self.x = 1\n",
    "        self.p = [i for i in range(p)]\n",
    "class B:\n",
    "    def __init__(self, a):\n",
    "        self.a = a\n",
    "        self.a.x = 2\n",
    "class C:\n",
    "    def __init__(self):\n",
    "        self.a = None\n",
    "    def aa(self, a):\n",
    "        self.a = a\n",
    "    def b(self):\n",
    "        self.a.p.append(3)\n",
    "\n",
    "a = A(4)\n",
    "print(\"Original: \", a.x, a.p)\n",
    "b = B(a)\n",
    "print(\"After B : \", a.x, a.p)\n",
    "c = C()\n",
    "c.aa(a)\n",
    "c.a.x += 4\n",
    "c.b()\n",
    "print(\"After C : \", a.x, a.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class X:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "f = X()\n",
    "g = X()\n",
    "h = X()\n",
    "\n",
    "j = [f,g]\n",
    "k = [f,g,h]\n",
    "\n",
    "for n in k:\n",
    "    if n not in j:\n",
    "        print(str(n) + \"not in k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "a = {} #prev\n",
    "b = {'1': 2, '2': 3}\n",
    "k = {'1': 1, '2': 4}\n",
    "\n",
    "def update(old,new):\n",
    "    upd = {}\n",
    "    for t, q in new.items():\n",
    "        if t not in old.keys():\n",
    "            upd[t] = q\n",
    "        elif upd[t] < q:\n",
    "            upd[t] = q\n",
    "    print(upd)\n",
    "            \n",
    "\n",
    "print(update(b, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"a;b;c;\"\n",
    "v = a.split(';')\n",
    "v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "j = {'a':1,  'b':2}\n",
    "j = str(j)\n",
    "k = json.loads(j.replace(\"\\'\",\"\\\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "b = [1,3,5]\n",
    "a.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a += b\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 98.917\n",
      "> 99.033\n"
     ]
    }
   ],
   "source": [
    "# deeper cnn model for mnist\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "\t# load dataset\n",
    "\t(trainX, trainY), (testX, testY) = mnist.load_data()\n",
    "\t# reshape dataset to have a single channel\n",
    "\ttrainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
    "\ttestX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
    "\t# one hot encode target values\n",
    "\ttrainY = to_categorical(trainY)\n",
    "\ttestY = to_categorical(testY)\n",
    "\treturn trainX, trainY, testX, testY\n",
    "\n",
    "# scale pixels\n",
    "def prep_pixels(train, test):\n",
    "\t# convert from integers to floats\n",
    "\ttrain_norm = train.astype('float32')\n",
    "\ttest_norm = test.astype('float32')\n",
    "\t# normalize to range 0-1\n",
    "\ttrain_norm = train_norm / 255.0\n",
    "\ttest_norm = test_norm / 255.0\n",
    "\t# return normalized images\n",
    "\treturn train_norm, test_norm\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(Dense(10, activation='softmax'))\n",
    "\t# compile model\n",
    "\topt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "# evaluate a model using k-fold cross-validation\n",
    "def evaluate_model(dataX, dataY, n_folds=5):\n",
    "\tscores, histories = list(), list()\n",
    "\t# prepare cross validation\n",
    "\tkfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "\t# enumerate splits\n",
    "\tfor train_ix, test_ix in kfold.split(dataX):\n",
    "\t\t# define model\n",
    "\t\tmodel = define_model()\n",
    "\t\t# select rows for train and test\n",
    "\t\ttrainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]\n",
    "\t\t# fit model\n",
    "\t\thistory = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=0)\n",
    "\t\t# evaluate model\n",
    "\t\t_, acc = model.evaluate(testX, testY, verbose=0)\n",
    "\t\tprint('> %.3f' % (acc * 100.0))\n",
    "\t\t# stores scores\n",
    "\t\tscores.append(acc)\n",
    "\t\thistories.append(history)\n",
    "\treturn scores, histories\n",
    "\n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(histories):\n",
    "\tfor i in range(len(histories)):\n",
    "\t\t# plot loss\n",
    "\t\tplt.subplot(2, 1, 1)\n",
    "\t\tplt.title('Cross Entropy Loss')\n",
    "\t\tplt.plot(histories[i].history['loss'], color='blue', label='train')\n",
    "\t\tplt.plot(histories[i].history['val_loss'], color='orange', label='test')\n",
    "\t\t# plot accuracy\n",
    "\t\tplt.subplot(2, 1, 2)\n",
    "\t\tplt.title('Classification Accuracy')\n",
    "\t\tplt.plot(histories[i].history['accuracy'], color='blue', label='train')\n",
    "\t\tplt.plot(histories[i].history['val_accuracy'], color='orange', label='test')\n",
    "\tplt.show()\n",
    "\n",
    "# summarize model performance\n",
    "def summarize_performance(scores):\n",
    "\t# print summary\n",
    "\tprint('Accuracy: mean=%.3f std=%.3f, n=%d' % (mean(scores)*100, std(scores)*100, len(scores)))\n",
    "\t# box and whisker plots of results\n",
    "\tplt.boxplot(scores)\n",
    "\tplt.show()\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# load dataset\n",
    "\ttrainX, trainY, testX, testY = load_dataset()\n",
    "\t# prepare pixel data\n",
    "\ttrainX, testX = prep_pixels(trainX, testX)\n",
    "\t# evaluate model\n",
    "\tscores, histories = evaluate_model(trainX, trainY)\n",
    "\t# learning curves\n",
    "\tsummarize_diagnostics(histories)\n",
    "\t# summarize estimated performance\n",
    "\tsummarize_performance(scores)\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(trainX, trainY, epochs=10, batch_size=32, verbose=0)\n",
    "model.save('final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "p = []\n",
    "for subdir, dirs, files in os.walk(\"./dataset/tser\"):\n",
    "    for filename in files:\n",
    "        filepath = subdir + '/' + filename\n",
    "        p.append(filepath)\n",
    "\n",
    "for j in p:\n",
    "    k = pd.read_csv(j)\n",
    "    k = k.drop(columns=['start','end'])\n",
    "    k.to_csv(j[:-4]+'2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            load\n",
      "0      -0.763592\n",
      "1      -0.799169\n",
      "2      -0.821227\n",
      "3      -0.822650\n",
      "4      -0.952862\n",
      "...          ...\n",
      "195739 -1.614599\n",
      "195740 -1.642349\n",
      "195741 -1.704253\n",
      "195742 -1.754773\n",
      "195743 -1.803870\n",
      "\n",
      "[195744 rows x 1 columns]\n",
      "            load\n",
      "0      -0.056634\n",
      "1      -0.184627\n",
      "2      -0.243261\n",
      "3      -0.322631\n",
      "4      -0.346942\n",
      "...          ...\n",
      "195739 -1.654043\n",
      "195740 -1.533916\n",
      "195741 -1.577533\n",
      "195742 -1.608280\n",
      "195743 -1.744854\n",
      "\n",
      "[195744 rows x 1 columns]\n",
      "           load\n",
      "0      0.878698\n",
      "1      0.886423\n",
      "2      0.645988\n",
      "3      0.589983\n",
      "4      0.270369\n",
      "...         ...\n",
      "48931 -0.573567\n",
      "48932 -0.601570\n",
      "48933 -0.616054\n",
      "48934 -0.945324\n",
      "48935 -1.314184\n",
      "\n",
      "[48936 rows x 1 columns]\n",
      "            load\n",
      "0      -1.352670\n",
      "1      -1.392351\n",
      "2      -1.426064\n",
      "3      -1.468131\n",
      "4      -1.481855\n",
      "...          ...\n",
      "198716 -1.341930\n",
      "198717 -1.361024\n",
      "198718 -1.400804\n",
      "198719 -1.397323\n",
      "198720 -1.449733\n",
      "\n",
      "[198714 rows x 1 columns]\n",
      "           load\n",
      "0     -0.750797\n",
      "1     -0.900565\n",
      "2     -1.062477\n",
      "3     -1.125893\n",
      "4     -1.113749\n",
      "...         ...\n",
      "48929 -0.559201\n",
      "48930 -0.708969\n",
      "48931 -1.015253\n",
      "48932 -1.287805\n",
      "48933 -1.447018\n",
      "\n",
      "[48934 rows x 1 columns]\n",
      "           load\n",
      "0     -0.880640\n",
      "1     -1.235931\n",
      "2     -1.548105\n",
      "3     -1.768437\n",
      "4     -1.845834\n",
      "...         ...\n",
      "48897  1.164442\n",
      "48898  0.920826\n",
      "48899  0.459033\n",
      "48900  0.178120\n",
      "48901 -0.187088\n",
      "\n",
      "[48902 rows x 1 columns]\n",
      "           load\n",
      "0      1.487074\n",
      "1      1.388122\n",
      "2      1.100854\n",
      "3      0.909542\n",
      "4      0.881894\n",
      "...         ...\n",
      "48865 -0.646722\n",
      "48866 -0.515414\n",
      "48867 -0.533647\n",
      "48868 -0.920294\n",
      "48869 -1.158772\n",
      "\n",
      "[48870 rows x 1 columns]\n",
      "           load\n",
      "0     -1.005909\n",
      "1     -1.051966\n",
      "2     -0.950641\n",
      "3     -1.050452\n",
      "4     -1.233671\n",
      "...         ...\n",
      "97809 -1.092976\n",
      "97810 -1.248813\n",
      "97811 -1.417142\n",
      "97812 -1.483767\n",
      "97813 -1.590771\n",
      "\n",
      "[97814 rows x 1 columns]\n",
      "           load\n",
      "0     -0.469947\n",
      "1     -0.585938\n",
      "2     -0.759108\n",
      "3     -0.915940\n",
      "4     -1.147922\n",
      "...         ...\n",
      "97143  0.647486\n",
      "97144  0.668724\n",
      "97145  0.588674\n",
      "97146  0.660555\n",
      "97147  0.707932\n",
      "\n",
      "[97148 rows x 1 columns]\n",
      "           load\n",
      "0     -1.277766\n",
      "1     -1.486052\n",
      "2     -1.658098\n",
      "3     -1.756092\n",
      "4     -1.754130\n",
      "...         ...\n",
      "48931  1.453899\n",
      "48932  1.052896\n",
      "48933  0.616829\n",
      "48934  0.251804\n",
      "48935 -0.057485\n",
      "\n",
      "[48936 rows x 1 columns]\n",
      "            load\n",
      "0      -0.449579\n",
      "1      -0.484425\n",
      "2      -0.536695\n",
      "3      -0.580254\n",
      "4      -0.615100\n",
      "...          ...\n",
      "198716 -0.170805\n",
      "198717 -0.257922\n",
      "198718 -0.318904\n",
      "198719 -0.371174\n",
      "198720 -0.414732\n",
      "\n",
      "[198593 rows x 1 columns]\n",
      "            load\n",
      "0      -1.284933\n",
      "1      -1.327386\n",
      "2      -1.378069\n",
      "3      -1.436982\n",
      "4      -1.490264\n",
      "...          ...\n",
      "195739 -0.488734\n",
      "195740 -0.574505\n",
      "195741 -0.613925\n",
      "195742 -0.632986\n",
      "195743 -0.683235\n",
      "\n",
      "[195744 rows x 1 columns]\n",
      "           load\n",
      "0      0.103936\n",
      "1     -0.010191\n",
      "2     -0.079415\n",
      "3     -0.119328\n",
      "4     -0.073178\n",
      "...         ...\n",
      "48814 -1.119024\n",
      "48815 -1.124637\n",
      "48816 -1.208205\n",
      "48817 -1.333245\n",
      "48818 -1.417124\n",
      "\n",
      "[48819 rows x 1 columns]\n",
      "           load\n",
      "0     -0.325307\n",
      "1     -0.543871\n",
      "2     -0.899415\n",
      "3     -1.228772\n",
      "4     -1.439278\n",
      "...         ...\n",
      "48931  0.669813\n",
      "48932  0.678878\n",
      "48933  0.280024\n",
      "48934 -0.172212\n",
      "48935 -0.544878\n",
      "\n",
      "[48936 rows x 1 columns]\n",
      "           load\n",
      "0     -0.252876\n",
      "1     -0.323942\n",
      "2     -0.379084\n",
      "3     -0.393533\n",
      "4     -0.387931\n",
      "...         ...\n",
      "48882 -1.092102\n",
      "48883 -1.155206\n",
      "48884 -1.358378\n",
      "48885 -1.519677\n",
      "48886 -1.601654\n",
      "\n",
      "[48887 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "p = []\n",
    "for subdir, dirs, files in os.walk(\"./dataset/tser/norm\"):\n",
    "    for filename in files:\n",
    "        filepath = subdir + '/' + filename\n",
    "        p.append(filepath)\n",
    "\n",
    "for j in p:\n",
    "    k = pd.read_csv(j)\n",
    "    k = k.drop(columns=['n'])\n",
    "    k = k.dropna()\n",
    "    #print(k.isna().sum())\n",
    "    k[['load']] = StandardScaler().fit_transform(k[['load']])\n",
    "    #arx = np.array(k)\n",
    "    #arx = preprocessing.normalize(arx)\n",
    "    print(k)\n",
    "    k.to_csv(j[:-4]+'n.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bb9c770599c29accaf2a9240ce1368d4cdf7f8298a578ed060299133d1d1d8bb"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
